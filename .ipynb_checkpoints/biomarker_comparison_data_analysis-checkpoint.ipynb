{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jeRM7RF2Ry_z"
   },
   "source": [
    "# Data Import and Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1F5DsKF8APKi",
    "outputId": "878339f3-f4ff-4f2f-bc75-fb7c2d39b261"
   },
   "outputs": [],
   "source": [
    "#system\n",
    "import warnings\n",
    "import sys\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "import os\n",
    "from os import path\n",
    "from datetime import date\n",
    "\n",
    "# # scikit-misc for loess smoothing\n",
    "# required = {'outlier_utils','scikit-misc','scikit_posthocs'}\n",
    "# installed = {pkg.key for pkg in pkg_resources.working_set}\n",
    "# missing = required - installed\n",
    "\n",
    "# if missing:\n",
    "#     python = sys.executable\n",
    "#     subprocess.check_call([python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)\n",
    "# from scipy import misc\n",
    "\n",
    "#stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from pandas.api.types import CategoricalDtype\n",
    "import outliers\n",
    "from numpy import cov\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "#scale and center dataset\n",
    "#https://scikit-learn.org/stable/modules/preprocessing.html \n",
    "from sklearn import preprocessing\n",
    "\n",
    "#scipy\n",
    "from scipy import stats as sci\n",
    "from scipy.stats import linregress\n",
    "from scipy.stats.mstats import gmean\n",
    "from scipy.stats import gstd\n",
    "from scipy.stats import kendalltau\n",
    "from scipy.optimize import curve_fit\n",
    "import scikit_posthocs as sp\n",
    "\n",
    "#sklearn related\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.utils import resample as bootstrap\n",
    "import skmisc\n",
    "\n",
    "#plotting\n",
    "from plotnine import *\n",
    "import matplotlib\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as mplt\n",
    "%matplotlib inline\n",
    "from mizani.formatters import scientific_format\n",
    "from pylab import * #colormap extraction\n",
    "import seaborn as sns \n",
    "\n",
    "#scripts\n",
    "from reprocess_qpcr import *\n",
    "from calculations import *\n",
    "from read_gsheets import *\n",
    "from qa_qc import *\n",
    "\n",
    "#sheets\n",
    "rna_tab = 'sample_inventory'\n",
    "ww_tab='site_lookup'\n",
    "facility_lookup='site_lookup' #from Hannah: why are ww_tab and facility_lookup the same?\n",
    "qpcr_results_tab = 'QuantStudio_raw_data'\n",
    "qpcr_plates_tab = 'Plate_info'\n",
    "cases_tab='casedata'\n",
    "chem_data = 'metadata'\n",
    "master=\"master_curves\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving figures and tables\n",
    "dir=os.getcwd()\n",
    "dir=os.path.join(dir, \"Figures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#made this by editing code from here https://gist.github.com/AllenDowney/818f6153ef316aee80467c51faee80f8\n",
    "import statsmodels.api as sm\n",
    "from contextlib import suppress\n",
    "smlowess = sm.nonparametric.lowess\n",
    "\n",
    "# from skmisc.loess import loess as loess_klass \n",
    "\n",
    "# import warnings\n",
    "# from contextlib import suppress\n",
    "# a=a.sort_values(\"date_sampling\")\n",
    "\n",
    "def pnlowess(y,x, **params):\n",
    "    for k in ('is_sorted', 'return_sorted'):\n",
    "        with suppress(KeyError):\n",
    "            del params['method_args'][k]\n",
    "            warnings.warn(\n",
    "                \"Smoothing method argument: {}, \"\n",
    "                \"has been ignored.\".format(k)\n",
    "            )\n",
    "\n",
    "    result = smlowess(y, x,\n",
    "                      frac=params['span'],\n",
    "                      is_sorted=True,\n",
    "                      **params['method_args'])\n",
    "    data = pd.DataFrame({\n",
    "        'x': result[:, 0],\n",
    "        'y': result[:, 1]})\n",
    "    return(data)\n",
    "\n",
    "\n",
    "def make_loess(series, wk):\n",
    "    endog = series.values\n",
    "    exog = series.index.values\n",
    "    tot=len(endog)\n",
    "    fr=wk/tot\n",
    "\n",
    "    smooth = smlowess(endog, exog,frac=fr, is_sorted=True) #is_sorted needs to be true when working with dates (and need to make sure it is actually sorted before using this function)\n",
    "    index, data = np.transpose(smooth)\n",
    "    \n",
    "    return pd.Series(data, index=pd.to_datetime(index))\n",
    "\n",
    "\n",
    "\n",
    "#based on code from here https://stackoverflow.com/questions/25571882/pandas-columns-correlation-with-statistical-significance\n",
    "def calculate_pvalues(df_full):\n",
    "    final_ken=pd.DataFrame()\n",
    "    for loc in locs:\n",
    "      df=df_full[df_full.location==loc].copy()\n",
    "      df = df._get_numeric_data()\n",
    "      dfcols = pd.DataFrame(columns=df.columns)\n",
    "      values = dfcols.transpose().join(dfcols, how='outer')\n",
    "      for r in df.columns:\n",
    "          for c in df.columns:\n",
    "              values[r][c] = kendalltau(df[r], df[c], nan_policy='omit')[1]\n",
    "\n",
    "      values=values.reset_index()[values.index.isin(row1)].drop(row1,axis=1)\n",
    "      values[\"location\"]=loc\n",
    "      final_ken=final_ken.append(values)\n",
    "    return final_ken\n",
    "\n",
    "def calculate_kendall(df_full):\n",
    "    final_ken=pd.DataFrame()\n",
    "    for loc in locs:\n",
    "      df=df_full[df_full.location==loc].copy()\n",
    "      df = df._get_numeric_data()\n",
    "      dfcols = pd.DataFrame(columns=df.columns)\n",
    "      values = dfcols.transpose().join(dfcols, how='outer')\n",
    "      for r in df.columns:\n",
    "          for c in df.columns:\n",
    "              values[r][c] = round(kendalltau(df[r], df[c], nan_policy='omit')[0], 4)\n",
    "\n",
    "      values=values.reset_index()[values.index.isin(row1)].drop(row1,axis=1)\n",
    "      values[\"location\"]=loc\n",
    "      final_ken=final_ken.append(values)\n",
    "    return final_ken\n",
    "\n",
    "def calculate_kendalldt(df_full):\n",
    "    final_ken=pd.DataFrame()\n",
    "    for loc in locs:\n",
    "      df=df_full[df_full.date_type==loc].copy()\n",
    "      df = df._get_numeric_data()\n",
    "      dfcols = pd.DataFrame(columns=df.columns)\n",
    "      values = dfcols.transpose().join(dfcols, how='outer')\n",
    "      for r in df.columns:\n",
    "          for c in df.columns:\n",
    "              values[r][c] = round(kendalltau(df[r], df[c], nan_policy='omit')[0], 4)\n",
    "\n",
    "      values=values.reset_index()[values.index.isin(row1)].drop(row1,axis=1)\n",
    "      values[\"date_type\"]=loc\n",
    "      final_ken=final_ken.append(values)\n",
    "    return final_ken\n",
    "\n",
    "\n",
    "def dataframes_kendall(wBDL, woBDL, condition, lis_t2):\n",
    "  def kendall_pval(x,y):\n",
    "          return kendalltau(x,y)[1]\n",
    "\n",
    "  def kendall_kval(x,y):\n",
    "          return kendalltau(x,y)[0]\n",
    "  wBDL_pval=wBDL.groupby(['location']).corr(method=kendall_pval).reset_index()\n",
    "  wBDL_pval=wBDL_pval[wBDL_pval.level_1!= n_new_sm].drop(lis_t2, axis=1)\n",
    "  wBDL_pval.columns=[\"location\", \"Target\", \"pval\"]\n",
    "  wBDL_kval=wBDL.groupby(['location']).corr(method=kendall_kval).reset_index()\n",
    "  wBDL_kval=wBDL_kval[wBDL_kval.level_1!= n_new_sm].drop(lis_t2, axis=1)\n",
    "  wBDL_kval.columns=[\"location\", \"Target\", \"kval\"]\n",
    "  wBDL=wBDL_kval.merge(wBDL_pval)\n",
    "  wBDL[\"BDL\"]=\"BLoD\"\n",
    "  wBDL[\"condition\"]= wBDL[\"location\"]+\" \"+condition\n",
    "  woBDL_pval=woBDL.groupby(['location']).corr(method=kendall_pval).reset_index()\n",
    "  woBDL_pval=woBDL_pval[woBDL_pval.level_1!= n_new_sm].drop(lis_t2, axis=1)\n",
    "  woBDL_pval.columns=[\"location\", \"Target\", \"pval\"]\n",
    "  woBDL_kval=woBDL.groupby(['location']).corr(method=kendall_kval).reset_index()\n",
    "  woBDL_kval=woBDL_kval[woBDL_kval.level_1!= n_new_sm].drop(lis_t2, axis=1)\n",
    "  woBDL_kval.columns=[\"location\", \"Target\", \"kval\"]\n",
    "  woBDL=woBDL_kval.merge(woBDL_pval)\n",
    "  woBDL[\"BDL\"]=\"without  BLoD\"\n",
    "  woBDL[\"condition\"]= woBDL[\"location\"]+\" \"+condition\n",
    "  fin=wBDL.append(woBDL).reset_index(drop=True)\n",
    "  return(fin)\n",
    "\n",
    "def dataframes_kendalldt(wBDL, woBDL, condition,lis_t2):\n",
    "  def kendall_pval(x,y):\n",
    "          return kendalltau(x,y)[1]\n",
    "\n",
    "  def kendall_kval(x,y):\n",
    "          return kendalltau(x,y)[0]\n",
    "  wBDL_pval=wBDL.groupby(['date_type']).corr(method=kendall_pval).reset_index()\n",
    "  wBDL_pval=wBDL_pval[wBDL_pval.level_1!= n_new_sm].drop(lis_t2, axis=1)\n",
    "  wBDL_pval.columns=[\"date_type\", \"Target\", \"pval\"]\n",
    "  wBDL_kval=wBDL.groupby(['date_type']).corr(method=kendall_kval).reset_index()\n",
    "  wBDL_kval=wBDL_kval[wBDL_kval.level_1!= n_new_sm].drop(lis_t2, axis=1)\n",
    "  wBDL_kval.columns=[\"date_type\", \"Target\", \"kval\"]\n",
    "  wBDL=wBDL_kval.merge(wBDL_pval)\n",
    "  wBDL[\"BDL\"]=\"BLoD\"\n",
    "  wBDL[\"condition\"]= wBDL[\"date_type\"]+\" \"+condition\n",
    "  woBDL_pval=woBDL.groupby(['date_type']).corr(method=kendall_pval).reset_index()\n",
    "  woBDL_pval=woBDL_pval[woBDL_pval.level_1!= n_new_sm].drop(lis_t2, axis=1)\n",
    "  woBDL_pval.columns=[\"date_type\", \"Target\", \"pval\"]\n",
    "  woBDL_kval=woBDL.groupby(['date_type']).corr(method=kendall_kval).reset_index()\n",
    "  woBDL_kval=woBDL_kval[woBDL_kval.level_1!= n_new_sm].drop(lis_t2, axis=1)\n",
    "  woBDL_kval.columns=[\"date_type\", \"Target\", \"kval\"]\n",
    "  woBDL=woBDL_kval.merge(woBDL_pval)\n",
    "  woBDL[\"BDL\"]=\"without  BLoD\"\n",
    "  woBDL[\"condition\"]= woBDL[\"date_type\"]+\" \"+condition\n",
    "  fin=wBDL.append(woBDL).reset_index(drop=True)\n",
    "  return(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', '{:.2E}'.format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owner/Desktop/Berkeley_Work/git_repos/2020_bay_area_COVID-19_WBE_manuscript/read_gsheets.py:42: UserWarning: \n",
      "\n",
      "\n",
      " 1 samples are double listed in sample tracking spreadsheet. Check the following samples:\n",
      "\n",
      "\n",
      "[nan]\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMMoV\n",
      "   Quantity  Cq_mean  negatives  total   fr_pos\n",
      "0  1.00E+02 3.64E+01          0     12 1.00E+00\n",
      "1  1.00E+03 3.33E+01          0     12 1.00E+00\n",
      "2  1.00E+04 2.98E+01          0     12 1.00E+00\n",
      "3  1.00E+05 2.64E+01          0     12 1.00E+00\n",
      "4  1.00E+06 2.26E+01          0     12 1.00E+00\n",
      "5  1.00E+07 1.90E+01          0     12 1.00E+00\n",
      "6  1.00E+08 1.57E+01          0     12 1.00E+00\n",
      "N1\n",
      "    Quantity  Cq_mean  negatives  total   fr_pos\n",
      "0   3.12E-01 3.83E+01         11     12 8.33E-02\n",
      "1   6.25E-01 3.83E+01         11     12 8.33E-02\n",
      "2   1.25E+00 3.82E+01          9     12 2.50E-01\n",
      "3   2.50E+00 3.78E+01          6     12 5.00E-01\n",
      "4   5.00E+00 3.74E+01         18     53 6.60E-01\n",
      "5   1.00E+01 3.64E+01          5     51 9.02E-01\n",
      "6   2.00E+01 3.55E+01          1     54 9.81E-01\n",
      "7   1.00E+02 3.29E+01          1     54 9.81E-01\n",
      "8   1.00E+03 2.93E+01          0     54 1.00E+00\n",
      "9   1.00E+04 2.59E+01          0     54 1.00E+00\n",
      "10  1.00E+05 2.26E+01          1     54 9.81E-01\n",
      "Xeno\n",
      "    Quantity  Cq_mean  negatives  total   fr_pos\n",
      "0   3.12E-01 3.22E+01          1     12 9.17E-01\n",
      "1   6.25E-01 3.23E+01          0     12 1.00E+00\n",
      "2   1.25E+00 3.23E+01          0     12 1.00E+00\n",
      "3   2.50E+00 3.25E+01          0     12 1.00E+00\n",
      "4   5.00E+00 3.22E+01          0     54 1.00E+00\n",
      "5   1.00E+01 3.23E+01          0     54 1.00E+00\n",
      "6   2.00E+01 3.23E+01          0     54 1.00E+00\n",
      "7   1.00E+02 3.24E+01          0     54 1.00E+00\n",
      "8   1.00E+03 3.20E+01          0     54 1.00E+00\n",
      "9   1.00E+04 3.20E+01          0     54 1.00E+00\n",
      "10  1.00E+05 3.36E+01          8     54 8.52E-01\n",
      "18S\n",
      "   Quantity  Cq_mean  negatives  total   fr_pos\n",
      "0  1.00E+02 3.52E+01          0      9 1.00E+00\n",
      "1  1.00E+03 3.17E+01          0      9 1.00E+00\n",
      "2  1.00E+04 2.99E+01          1      9 8.89E-01\n",
      "3  1.00E+05 2.60E+01          1      9 8.89E-01\n",
      "4  1.00E+06 2.26E+01          1      9 8.89E-01\n",
      "5  1.00E+07 1.68E+01          0      9 1.00E+00\n",
      "6  1.00E+08 1.32E+01          0      9 1.00E+00\n",
      "bact\n",
      "    Quantity  Cq_mean  negatives  total   fr_pos\n",
      "0   1.00E+02 3.62E+01          0      6 1.00E+00\n",
      "1   2.00E+02 3.47E+01          0      3 1.00E+00\n",
      "2   1.00E+03 3.29E+01          0      6 1.00E+00\n",
      "3   2.00E+03 3.13E+01          0      3 1.00E+00\n",
      "4   1.00E+04 2.94E+01          0      6 1.00E+00\n",
      "5   2.00E+04 2.79E+01          0      3 1.00E+00\n",
      "6   1.00E+05 2.58E+01          0      6 1.00E+00\n",
      "7   2.00E+05 2.45E+01          0      3 1.00E+00\n",
      "8   1.00E+06 2.19E+01          0      6 1.00E+00\n",
      "9   2.00E+06 2.06E+01          0      3 1.00E+00\n",
      "10  1.00E+07 1.79E+01          0      6 1.00E+00\n",
      "11  2.00E+07 1.70E+01          0      3 1.00E+00\n",
      "12  1.00E+08 1.42E+01          0      6 1.00E+00\n",
      "13  2.00E+08 1.34E+01          0      3 1.00E+00\n",
      "crAss\n",
      "   Quantity  Cq_mean  negatives  total   fr_pos\n",
      "0  1.00E+03 3.30E+01          0     27 1.00E+00\n",
      "1  1.00E+04 2.97E+01          0     27 1.00E+00\n",
      "2  1.00E+05 2.61E+01          0     27 1.00E+00\n",
      "3  1.00E+06 2.25E+01          0     27 1.00E+00\n",
      "4  1.00E+07 1.87E+01          0     27 1.00E+00\n",
      "5  1.00E+08 1.52E+01          0     27 1.00E+00\n",
      "6  1.00E+09 1.19E+01          0     27 1.00E+00\n",
      "GFP\n",
      "   Quantity  Cq_mean  negatives  total   fr_pos\n",
      "0  1.00E+04 2.80E+01          0      6 1.00E+00\n",
      "1  1.00E+05 2.45E+01          0      6 1.00E+00\n",
      "2  1.00E+06 2.10E+01          0      6 1.00E+00\n",
      "3  1.00E+07 1.72E+01          0      6 1.00E+00\n",
      "4  1.00E+08 1.38E+01          0      6 1.00E+00\n",
      "5  1.00E+09 1.04E+01          0      6 1.00E+00\n",
      "6  1.00E+10 6.97E+00          0      6 1.00E+00\n",
      "bCoV\n",
      "   Quantity  Cq_mean  negatives  total   fr_pos\n",
      "0  1.00E+01 3.86E+01          2      3 3.33E-01\n",
      "1  2.00E+01 4.08E+01          2      3 3.33E-01\n",
      "2  1.00E+02 3.96E+01          1      6 8.33E-01\n",
      "3  1.00E+03 3.57E+01          0      6 1.00E+00\n",
      "4  1.00E+04 3.19E+01          0      6 1.00E+00\n",
      "5  1.00E+05 2.87E+01          0      6 1.00E+00\n",
      "6  1.00E+06 2.41E+01          0      6 1.00E+00\n",
      "7  1.00E+07 2.04E+01          0      3 1.00E+00\n",
      "8  1.00E+08 1.65E+01          0      3 1.00E+00\n"
     ]
    }
   ],
   "source": [
    "# metadata\n",
    "sample_data = read_sample_data(rna_tab, facility_lookup)\n",
    "copy_sd=sample_data.copy()\n",
    "\n",
    "sample_data[\"RNA_yield_quantification\"]=np.nan\n",
    "sample_data[\"DNA_yield_quantification\"]=np.nan\n",
    "sample_data.loc[sample_data.RNA_ng_ul_qubit_all=='<5','RNA_yield_quantification']='below quantification limit'\n",
    "sample_data.loc[(sample_data.RNA_ng_ul_qubit_all!=\"\")&(sample_data.RNA_ng_ul_qubit_all!='<5'),'RNA_yield_quantification']='quantifiable'\n",
    "sample_data.loc[sample_data.RNA_ng_ul_qubit_all=='<5','RNA_ng_ul_qubit_all']=2.5\n",
    "sample_data.loc[sample_data.DNA_ng_ul_qubit_all=='<0.05','DNA_yield_quantification']='below quantification limit'\n",
    "sample_data.loc[((sample_data.DNA_ng_ul_qubit_all!=\"\"))&(sample_data.DNA_ng_ul_qubit_all!='<0.05'),'DNA_yield_quantification']='quantifiable'\n",
    "sample_data.loc[sample_data.DNA_ng_ul_qubit_all=='<0.05','DNA_ng_ul_qubit_all']=0.025\n",
    "\n",
    "#get RNA data into editable form\n",
    "sample_data['elution_vol_ul']=pd.to_numeric(sample_data.elution_vol_ul)\n",
    "sample_data['weight_vol_extracted_ml']=pd.to_numeric(sample_data.weight_vol_extracted_ml)\n",
    "sample_data['RNA_ng_ul_qubit_all']=pd.to_numeric(sample_data.RNA_ng_ul_qubit_all)\n",
    "sample_data['DNA_ng_ul_qubit_all']=pd.to_numeric(sample_data.DNA_ng_ul_qubit_all)\n",
    "sample_data['RNA_yield_ext']=(sample_data.RNA_ng_ul_qubit_all*sample_data.elution_vol_ul)\n",
    "sample_data['DNA_yield_ext']=(sample_data.DNA_ng_ul_qubit_all*sample_data.elution_vol_ul)\n",
    "sample_data['RNA_yield']=(sample_data.RNA_ng_ul_qubit_all*sample_data.elution_vol_ul)/sample_data.weight_vol_extracted_ml\n",
    "sample_data['DNA_yield']=(sample_data.DNA_ng_ul_qubit_all*sample_data.elution_vol_ul)/sample_data.weight_vol_extracted_ml\n",
    "sample_data['date_sampling']=pd.to_datetime(sample_data['date_sampling'])\n",
    "\n",
    "#Load master curve\n",
    "master_curve=pd.read_csv('data_files/'+master+'.csv').dropna(how='all')\n",
    "master_curve.LoD_Cq=pd.to_numeric(master_curve.LoD_Cq)\n",
    "master_curve.m=pd.to_numeric(master_curve.m)\n",
    "master_curve.b=pd.to_numeric(master_curve.b)\n",
    "master_curve.LoD_quantity=pd.to_numeric(master_curve.LoD_quantity)\n",
    "master_curve.lowest_quantity=pd.to_numeric(master_curve.lowest_quantity)\n",
    "\n",
    "# plotting LoD lines for each plot\n",
    "master_curve[\"LoD_gc_per_L\"]= (master_curve.lowest_quantity/5)*1000*200/50\n",
    "N1_LoD=master_curve.loc[master_curve.Target==\"N1\", \"LoD_gc_per_L\"].item()\n",
    "S18_LoD=master_curve.loc[master_curve.Target==\"18S\", \"LoD_gc_per_L\"].item()\n",
    "PMMoV_LoD=master_curve.loc[master_curve.Target==\"PMMoV\", \"LoD_gc_per_L\"].item()\n",
    "C_LoD=master_curve.loc[master_curve.Target==\"crAss\", \"LoD_gc_per_L\"].item()\n",
    "bact_LoD=master_curve.loc[master_curve.Target==\"bact\", \"LoD_gc_per_L\"].item()\n",
    "\n",
    "#qpcr data\n",
    "qpcr_raw = read_qpcr_data(qpcr_results_tab, qpcr_plates_tab)\n",
    "plates=[ 68, 85, 86, 87,88, 91, 92, 93, 94, 95, 96,97, 99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121, 123, 124,125,126,127 ]\n",
    "plates_LTM= [ 87,88,  92, 93, 94, 95, 96, 99,100,101,102,123, 124,125,126,127 ]\n",
    "qpcr_raw=qpcr_raw[qpcr_raw.plate_id.isin(plates)].copy()\n",
    "# qpcr_raw=qpcr_raw[qpcr_raw.LoD_testing != \"Y\"].copy()\n",
    "qpcr_processed, std_curve_df, dilution_expts_df,raw_outliers_flagged_df, control_df = process_qpcr_raw(qpcr_raw, 'grubbs_only', master=master_curve, use_master_curve=True)\n",
    "\n",
    "# merge with sample data\n",
    "qpcr_averaged = qpcr_processed.merge(sample_data, how='left', left_on='Sample', right_on='sample_id')\n",
    "qpcr_averaged = qpcr_averaged[(qpcr_averaged.Sample != 'NTC') &\n",
    "                              (qpcr_averaged.Target != 'Xeno') &\n",
    "                              (qpcr_averaged.inhibition_testing == \"N\")] # Remove spike and dilute inhibition testing from qPCR averaged\n",
    "\n",
    "# calculations\n",
    "qpcr_averaged['gc_per_L'] = calculate_gc_per_l(qpcr_averaged) # get gc/L\n",
    "qpcr_averaged = normalize_to_pmmov(qpcr_averaged)\n",
    "qpcr_averaged_merged= get_extraction_control(qpcr_averaged)\n",
    "und_N1=qpcr_averaged_merged[qpcr_averaged_merged.Target==\"N1\"][[\"is_undetermined_count\", \"replicate_init_count\", \"Sample\"]].copy()\n",
    "und_N1.columns=[\"is_undetermined_N1\", \"is_undetermined_N1_total\", \"Sample\"]\n",
    "bloq_N1=qpcr_averaged_merged[qpcr_averaged_merged.Target==\"N1\"][[\"is_bloq_count\", \"replicate_init_count\",  \"gc_per_L\",\"Sample\"]].copy()\n",
    "bloq_N1[\"is_bloq_bio_N1\"]=False\n",
    "bloq_N1.loc[ (bloq_N1.gc_per_L < N1_LoD ), \"is_bloq_bio_N1\"]=True\n",
    "bloq_N1=bloq_N1.drop(\"gc_per_L\", axis=1)\n",
    "bloq_N1.columns=[\"is_bloq_N1\", \"is_bloq_N1_total\",\"Sample\",\"is_bloq_bio_N1\"]\n",
    "\n",
    "qpcr_averaged_merged=qpcr_averaged_merged.merge(und_N1, how='left')\n",
    "qpcr_averaged_merged=qpcr_averaged_merged.merge(bloq_N1, how='left')\n",
    "\n",
    "# test inhibition\n",
    "qpcr_averaged_merged, xeno_inhib_full,xeno_control=xeno_inhibition_test(raw_outliers_flagged_df,qpcr_averaged_merged)\n",
    "xeno_inhib=xeno_inhib_full.merge(sample_data, left_on='Sample', right_on='sample_id',  how='left').copy()\n",
    "xeno_inhib=xeno_inhib[(xeno_inhib['batch'].str.contains(\"B\", na=False))].copy()\n",
    "x_plates=xeno_inhib.plate_id.unique()\n",
    "xeno_control=xeno_control[xeno_control.plate_id.isin(x_plates)].copy()\n",
    "\n",
    "# #remove pop up lab batches\n",
    "# qpcr_averaged_merged=qpcr_averaged_merged[(qpcr_averaged_merged['batch'].str.contains(\"B\", na=False))].copy()\n",
    "# nI=[\"RV\",\"SD2\",\"SRWS\",\"SR\"]\n",
    "# qpcr_averaged_merged=qpcr_averaged_merged[~qpcr_averaged_merged.interceptor.isin(nI)].copy()\n",
    "\n",
    "#control df\n",
    "control_df.loc[control_df.Task!=\"Negative Control\", \"Task\"]=\"PBS control\"\n",
    "control_df=control_df[control_df.plate_id.isin(plates) ].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2, 10])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "4S_Paper_figures_geopandas_local",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "py3.6.8_outlier_utils",
   "language": "python",
   "name": "py3.6.8_outlierutils"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
